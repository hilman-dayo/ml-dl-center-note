#+title: 交差エントロピー誤差

* 概念
  - Softmax Classifier (SC) と使われると考えればよいでしょう
  - CNN 上では Hinge Loss (HL) よりよく使われている損失関数
    - HL は余裕を算出する一方, 

* タイプ
** Cross Entropy Loss
   - 注意:
     - pytorch の cross entropy loss に注意
** Binary Cross Entropy
   - sigmoid と使う

** Categorical Cross Entropy
   - softmax loss とも言う
   - softmax activation + cross entropy loss
   - 複数クラス認識のため

* 実装
  =-log(softmax_classifer_result)=

* 参照
  - https://ja.wikipedia.org/wiki/%E4%BA%A4%E5%B7%AE%E3%82%A8%E3%83%B3%E3%83%88%E3%83%AD%E3%83%94%E3%83%BC
  - https://stats.stackexchange.com/questions/204484/what-are-the-differences-between-logistic-function-and-sigmoid-function
  - https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a
  - 良さそう
    - https://gombru.github.io/2018/05/23/cross_entropy_loss/
