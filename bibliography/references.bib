@techreport{active-learning-literature-survey,
  Author =       {Burr Settles},
  Institution =  {University of Wisconsin--Madison},
  Number =       1648,
  Title =        {Active Learning Literature Survey},
  Type =         {Computer Sciences Technical Report},
  Year =         2009,
}
@article{bdddataset,
  author =       {Yu, Fisher and Chen, Haofeng and Wang, Xin and Xian, Wenqi and
                  Chen, Yingying and Liu, Fangchen and Madhavan, Vashisht and
                  Darrell, Trevor},
  title =        {Bdd100k: a Diverse Driving Dataset for Heterogeneous Multitask
                  Learning},
  journal =      {CoRR},
  year =         2018,
  url =          {http://arxiv.org/abs/1805.04687v2},
  abstract =     {Datasets drive vision progress, yet existing driving datasets
                  are impoverished in terms of visual content and supported
                  tasks to study multitask learning for autonomous driving.
                  Researchers are usually constrained to study a small set of
                  problems on one dataset, while real-world computer vision
                  applications require performing tasks of various complexities.
                  We construct BDD100K, the largest driving video dataset with
                  100K videos and 10 tasks to evaluate the exciting progress of
                  image recognition algorithms on autonomous driving. The
                  dataset possesses geographic, environmental, and weather
                  diversity, which is useful for training models that are less
                  likely to be surprised by new conditions. Based on this
                  diverse dataset, we build a benchmark for heterogeneous
                  multitask learning and study how to solve the tasks together.
                  Our experiments show that special training strategies are
                  needed for existing models to perform such heterogeneous
                  tasks. BDD100K opens the door for future studies in this
                  important venue.},
  archivePrefix ={arXiv},
  eprint =       {1805.04687v2},
  primaryClass = {cs.CV},
}
@inproceedings{alexnet,
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
  added-at = {2017-02-26T17:53:42.000+0100},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  biburl = {https://www.bibsonomy.org/bibtex/2784f6d0ddce5f78d5d2105a1781cecc2/nosebrain},
  booktitle = {Advances in neural information processing systems},
  interhash = {74bbb5dea5afb1b088bd10e317f1f0d2},
  intrahash = {784f6d0ddce5f78d5d2105a1781cecc2},
  keywords = {classification cnn image imagenet},
  pages = {1097--1105},
  timestamp = {2017-02-26T17:54:48.000+0100},
  title = {Imagenet classification with deep convolutional neural networks},
  year = 2012
}

@article{googlenet,
  author =       {Szegedy, Christian and Liu, Wei and Jia, Yangqing and
                  Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and
                  Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  title =        {Going Deeper With Convolutions},
  journal =      {CoRR},
  year =         2014,
  url =          {http://arxiv.org/abs/1409.4842v1},
  abstract =     {We propose a deep convolutional neural network architecture
                  codenamed "Inception", which was responsible for setting the
                  new state of the art for classification and detection in the
                  ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC
                  2014). The main hallmark of this architecture is the improved
                  utilization of the computing resources inside the network.
                  This was achieved by a carefully crafted design that allows
                  for increasing the depth and width of the network while
                  keeping the computational budget constant. To optimize
                  quality, the architectural decisions were based on the Hebbian
                  principle and the intuition of multi-scale processing. One
                  particular incarnation used in our submission for ILSVRC 2014
                  is called GoogLeNet, a 22 layers deep network, the quality of
                  which is assessed in the context of classification and
                  detection.},
  archivePrefix ={arXiv},
  eprint =       {1409.4842v1},
  primaryClass = {cs.CV},
}
@article{vgg16,
  author =       {Simonyan, Karen and Zisserman, Andrew},
  title =        {Very Deep Convolutional Networks for Large-Scale Image
                  Recognition},
  journal =      {CoRR},
  year =         2014,
  url =          {http://arxiv.org/abs/1409.1556v5},
  abstract =     {In this work we investigate the effect of the convolutional
                  network depth on its accuracy in the large-scale image
                  recognition setting. Our main contribution is a thorough
                  evaluation of networks of increasing depth using an
                  architecture with very small (3x3) convolution filters, which
                  shows that a significant improvement on the prior-art
                  configurations can be achieved by pushing the depth to 16-19
                  weight layers. These findings were the basis of our ImageNet
                  Challenge 2014 submission, where our team secured the first
                  and the second places in the localisation and classification
                  tracks respectively. We also show that our representations
                  generalise well to other datasets, where they achieve
                  state-of-the-art results. We have made our two best-performing
                  ConvNet models publicly available to facilitate further
                  research on the use of deep visual representations in computer
                  vision.},
  archivePrefix ={arXiv},
  eprint =       {1409.1556v5},
  primaryClass = {cs.CV},
}
@article{resnet,
  author =       {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun,
                  Jian},
  title =        {Deep Residual Learning for Image Recognition},
  journal =      {CoRR},
  year =         2015,
  url =          {http://arxiv.org/abs/1512.03385v1},
  abstract =     {Deeper neural networks are more difficult to train. We present
                  a residual learning framework to ease the training of networks
                  that are substantially deeper than those used previously. We
                  explicitly reformulate the layers as learning residual
                  functions with reference to the layer inputs, instead of
                  learning unreferenced functions. We provide comprehensive
                  empirical evidence showing that these residual networks are
                  easier to optimize, and can gain accuracy from considerably
                  increased depth. On the ImageNet dataset we evaluate residual
                  nets with a depth of up to 152 layers---8x deeper than VGG
                  nets but still having lower complexity. An ensemble of these
                  residual nets achieves 3.57 \% error on the ImageNet test set.
                  This result won the 1st place on the ILSVRC 2015
                  classification task. We also present analysis on CIFAR-10 with
                  100 and 1000 layers. The depth of representations is of
                  central importance for many visual recognition tasks. Solely
                  due to our extremely deep representations, we obtain a 28 \%
                  relative improvement on the COCO object detection dataset.
                  Deep residual nets are foundations of our submissions to
                  ILSVRC \& COCO 2015 competitions, where we also won the 1st
                  places on the tasks of ImageNet detection, ImageNet
                  localization, COCO detection, and COCO segmentation.},
  archivePrefix ={arXiv},
  eprint =       {1512.03385},
  primaryClass = {cs.CV},
}
@article{how-much-real-data,
  author =       {Nowruzi, Farzan Erlik and Kapoor, Prince and Kolhatkar,
                  Dhanvin and Hassanat, Fahed Al and Laganiere, Robert and
                  Rebut, Julien},
  title =        {How Much Real Data Do We Actually Need: Analyzing Object
                  Detection Performance Using Synthetic and Real Data},
  journal =      {CoRR},
  year =         2019,
  url =          {http://arxiv.org/abs/1907.07061v1},
  abstract =     {In recent years, deep learning models have resulted in a huge
                  amount of progress in various areas, including computer
                  vision. By nature, the supervised training of deep models
                  requires a large amount of data to be available. This ideal
                  case is usually not tractable as the data annotation is a
                  tremendously exhausting and costly task to perform. An
                  alternative is to use synthetic data. In this paper, we take a
                  comprehensive look into the effects of replacing real data
                  with synthetic data. We further analyze the effects of having
                  a limited amount of real data. We use multiple synthetic and
                  real datasets along with a simulation tool to create large
                  amounts of cheaply annotated synthetic data. We analyze the
                  domain similarity of each of these datasets. We provide
                  insights about designing a methodological procedure for
                  training deep networks using these datasets.},
  archivePrefix ={arXiv},
  eprint =       {1907.07061},
  primaryClass = {cs.CV},
}
@article{chadwick19_train_objec_detec_with_noisy_data,
  author =       {Chadwick, Simon and Newman, Paul},
  title =        {Training Object Detectors With Noisy Data},
  journal =      {CoRR},
  year =         2019,
  url =          {http://arxiv.org/abs/1905.07202v1},
  abstract =     {The availability of a large quantity of labelled training data
                  is crucial for the training of modern object detectors. Hand
                  labelling training data is time consuming and expensive while
                  automatic labelling methods inevitably add unwanted noise to
                  the labels. We examine the effect of different types of label
                  noise on the performance of an object detector. We then show
                  how co-teaching, a method developed for handling noisy labels
                  and previously demonstrated on a classification problem, can
                  be improved to mitigate the effects of label noise in an
                  object detection setting. We illustrate our results using
                  simulated noise on the KITTI dataset and on a vehicle
                  detection task using automatically labelled data.},
  archivePrefix ={arXiv},
  eprint =       {1905.07202},
  primaryClass = {cs.RO},
}
