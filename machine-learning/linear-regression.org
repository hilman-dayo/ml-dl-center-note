#+property: header-args:python :session linear-r :async yes :kernel lightnet-dev
#+title: 線形回帰

これは Andrew Ng の Machine Learning コースの Exercise 1 (第 2 周目) の答え.

すべて Python で書いてある.

#+begin_src python :exports both
  import numpy as np
  import matplotlib.pyplot as plt
  from mpl_toolkits.mplot3d import Axes3D
  import pytest
#+end_src

#+RESULTS:


* 単回帰
** データの処理・探索
   #+name: data
   : ../datasets/coursera-ml/ex1/ex1data1.txt

   データの様子を見てみよう.
   #+begin_src bash :var data=data
     head $data -n 3
   #+end_src

   #+RESULTS:
   | 6.1101 | 17.592 |
   | 5.5277 | 9.1302 |
   | 8.5186 | 13.662 |


   確実にデータを読み込む.
   #+begin_src python :exports both :var data=data
     data = np.loadtxt(data, delimiter=",")
     X, y = data[:, 0], data[:, 1]
     X = np.c_[np.ones(len(X)), X]
     m = y.size

     def assert_with_answer(answer, gt, epsilon=0.0001):
         return pytest.approx(answer, epsilon) == gt
   #+end_src

   #+RESULTS:


   データを可視化.
   #+begin_src python :exports both :file ../output/images/cml-ex1-1.jpg
     def plot_data(x, y):
         # ms → markersize
         # mec markeredgecolor
         plt.plot(x, y, "ro", ms=10, mec="k")
         plt.ylabel("Profit in $10,000")
         plt.xlabel("Population of CIty in 10,000s")
     plot_data(X[:, 1], y)
   #+end_src

   #+RESULTS:
   [[file:../output/images/cml-ex1-1.jpg]]

** 計算
   #+name: h
   #+begin_src python :exports both
     def h(X, theta):
         return X @ theta
   #+end_src

   #+RESULTS:


    #+name: compute-cost
   #+begin_src python :exports both
      def compute_cost(X, y, theta, method=1):
          if method == 1:
              J = 1 / (2 * y.size) * np.square(h(X, theta) - y).sum()
          elif method == 2:
              # XXX: まだ試していない
              m = h(X, theta) - y
              J = 1 / (2 * y.size) * (m @ m)
          else:
              raise ValueError

          return J
   #+end_src


   #+begin_src python :exports both
     ans_0_0 = 32.07
     ans_n1_2 = 54.24
     print(assert_with_answer(
         compute_cost(X, y, np.array([0, 0])), ans_0_0
     ))
     print(assert_with_answer(
         compute_cost(X, y, np.array([-1, 2])), ans_n1_2
     ))
   #+end_src

   #+RESULTS:
   : True
   : True


   #+name: gradient-descent
   #+begin_src python :exports both
     def gradient_descent(X, y, theta, alpha, num_iters, method=1):
         m = y.size
         theta = theta.copy()
         J_history = []
         for i in range(num_iters):
             # 同じ計算が, 実装だけ違う
             if method == 1:
                 theta[:] -= (1 / m) * alpha * X.T @ (h(X, theta) - y)
             elif method == 2:
                 theta[:] -= (alpha /  m) * np.sum((h(X, theta) - y).reshape(-1, 1) * X, axis=0)
             else:
                 raise ValueError

             J_history.append(compute_cost(X, y, theta))

         return theta, J_history
   #+end_src


   #+begin_src python :exports both
     theta = np.zeros(2)
     iterations = 1500
     alpha = 0.01
     theta_out, J_history = gradient_descent(X, y, theta, alpha, iterations, method=1)
     ans = [-3.6303, 1.1664]
     print(assert_with_answer(theta_out, ans))

     # method 2 の実装も正しいと実証
     theta_out_2, J_history_2 = gradient_descent(X, y, theta, alpha, iterations, method=2)
     print(assert_with_answer(theta_out_2, theta_out))
     print(assert_with_answer(J_history_2, J_history))
   #+end_src

   #+RESULTS:
   : True
   : True
   : True


   本番の予測を行う.
   #+begin_src python :exports both
     # / 10,000 (そのスケールのため)
     ans_predict1 = 4519.7678677 / 10_000
     ans_predict2 = 45342.45012945 / 10_000

     print(assert_with_answer(h([1, 3.5], theta_out), ans_predict1))
     print(assert_with_answer(h([1, 7], theta_out), ans_predict2))
   #+end_src

   #+RESULTS:
   : True
   : True

** 可視化
   ロスを可視化.
   #+begin_src python :exports both :file ../output/images/lr-visualize.png
     plt.plot(J_history)
     plt.show()
   #+end_src

   #+RESULTS:
   [[file:../output/images/lr-visualize.png]]


   モデルを可視化.
   #+begin_src python :exports both :file ../output/images/gd-test.png
     plot_data(X[:, 1], y)
     plt.plot(X[:, 1], X @ theta_out, "-")
     plt.legend(["Training data", "Linear Regression"])
     plt.show()
   #+end_src

   #+RESULTS:
   [[file:../output/images/gd-test.png]]

   θ の可視化.
   #+begin_src python :exports both :file ../output/images/lr-3d-plot.png
     θ0 = np.linspace(-10, 10, 100)
     θ1 = np.linspace(-1, 4, 100)
     J_vals = np.zeros((θ0.size, θ1.size))

     for i, t1 in enumerate(θ0):
         for j, t2 in enumerate(θ1):
             J_vals[i, j] = compute_cost(X, y, [t1, t2])
     J_vals = J_vals.T

     fig = plt.figure(figsize=(12, 5))
     ax = fig.add_subplot(121, projection="3d")
     ax.plot_surface(θ0, θ1, J_vals, cmap="viridis")
     plt.xlabel("θ0")
     plt.ylabel("θ1")
     plt.title("Surfarce")

     ax = fig.add_subplot(122)
     plt.contour(θ0, θ1, J_vals, linewidths=2, cmap="viridis", levels=np.logspace(-2, 3, 20))
     plt.xlabel("θ0")
     plt.ylabel("θ1")
     plt.plot(theta_out[0], theta_out[1], "ro", ms=10, lw=2)
     plt.title("Contour, showing minimum")

     plt.show()
   #+end_src

   #+RESULTS:
   [[file:../output/images/lr-3d-plot.png]]

* Multiple Variables (複数?回来)
** データの処理・探索
   #+name: data2
   : ../datasets/coursera-ml/ex1/ex1data2.txt

   データの様子を見てみよう.
   #+begin_src bash :var data=data2
     head $data -n 3
   #+end_src

   #+RESULTS:
   | 2104 | 3 | 399900 |
   | 1600 | 3 | 329900 |
   | 2400 | 3 | 369000 |


   確実にデータを読み込む.
   #+begin_src python :exports both :var data=data2
     data = np.loadtxt(data, delimiter=",")
     X2, y2 = data[:, :2], data[:, 2]
     X2 = np.c_[np.ones(len(X2)), X2]
     m = y2.size

     def assert_with_answer(answer, gt, epsilon=0.0001):
         return pytest.approx(answer, epsilon) == gt
   #+end_src

   #+RESULTS:


   データを正規化.
   #+begin_src python :exports both
     def feature_normalize(X):
         "Standardization"
         X_norm = X.copy()
         mu = X_norm.mean(axis=0)[1:]
         sigma = X_norm.std(axis=0)[1:]

         for i in range(len(mu)):
             X_norm[:, i + 1] = (X_norm[:, i + 1] - mu[i]) / sigma[i]

         return X_norm, mu, sigma

     X_norm, mu, sigma = feature_normalize(X2)
   #+end_src

   #+RESULTS:

** 計算
   #+call: h()

   #+RESULTS:

   #+call: compute-cost()

   #+RESULTS:

   #+call: gradient-descent()

   #+RESULTS:


   良さそうな学習率を選択しよう.
   #+begin_src python :exports both :file ../output/images/lr-multi.png
     theta = np.zeros(X_norm.shape[1])
     iterations = 50
     # log of multiplicative of 3
     alphas = [0.3, 0.1, 0.03, 0.01]
     for alpha in alphas:
         theta_out, J_history = gradient_descent(X_norm, y2, theta, alpha, iterations, method=1)
         plt.plot(J_history, label=alpha)
     plt.legend()
     plt.show()
   #+end_src

   #+RESULTS:
   [[file:../output/images/lr-multi.png]]

   良い学習率を使って, 学習.
   #+begin_src python :exports both :var alpha=0.3
     num_iters = 400
     theta = np.zeros(X_norm.shape[1])
     theta, J_history = gradient_descent(X_norm, y2, theta, alpha, num_iters)
     print(f"Gradient Descent: {theta}")

     data_point = [1_650, 3]
     data_point_norm = np.r_[1, (data_point - mu) / sigma]
     price = h(data_point_norm, theta)
     print(f"Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): ${price:.0f}")
   #+end_src

   #+RESULTS:
   : Gradient Descent: [340412.65957447 109447.79646964  -6578.35485416]
   : Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): $293081

** Normal Equations
   #+begin_src python :exports both
     def normal_equation(X, y):
         return np.linalg.inv(X.T @ X) @ X.T @ y

     theta_ne = normal_equation(X2, y2)
     price_ne = h(np.r_[1, data_point], theta_ne)
     print(f"Normal equation: {theta_ne}")
     print(f"Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): ${price_ne:.0f}")
   #+end_src

   #+RESULTS:
   : Normal equation: [89597.9095428    139.21067402 -8738.01911233]
   : Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): $293081


   比較すると:
   #+begin_src python :exports both
     print(f"GD: {price}\nNE: {price_ne}")
   #+end_src

   #+RESULTS:
   : GD: 293081.46433489607
   : NE: 293081.4643348959


** 可視化
   ロスを可視化.
   #+begin_src python :exports both :file ../output/images/lrm-visualize.png
     plt.plot(J_history)
     plt.show()
   #+end_src

   #+RESULTS:
   [[file:../output/images/lrm-visualize.png]]

* 質問 [0/1]
  - [ ] 単回帰のときに, データを正規化しなくても大丈夫?
        大きな数字はアルゴリズムに影響を与えない?
  - [ ] normalize vs standardize を深く理解
* 参考
  - https://github.com/wavelets/ml-coursera-python-assignments/blob/master/Exercise1/exercise1.ipynb
  - https://nbviewer.ipython.org/github/JWarmenhoven/Machine-Learning/blob/master/notebooks/Programming%20Exercise%201%20-%20Linear%20Regression.ipynb

