#+property: header-args:python :session logistic-r :async yes :kernel lightnet-dev
#+title: ロジスティック回帰

これは Andrew Ng の Machine Learning コースの Exercise 2 (第 3 周目) の答え.
すべて Python で書いてある.

#+begin_src python
  import numpy as np
  import matplotlib.pyplot as plt
  from mpl_toolkits.mplot3d import Axes3D
  from scipy import optimize
  import pytest
#+end_src

#+RESULTS:


* 概念

** コスト関数

     \[
     Cost(h_{\theta}(x), y) = y * -log(h_{\theta}(x)) + (y - 1) * -log(1 - h_{\theta}(x))
     \]

   - 詳細は [[file:~/MY_DRIVE/STUDY/ml/ml_dl_center_note/cost-functions/logistic-regression-lost-function.org]]

* ロジスティック回帰
** データ

   #+name: data
   : ../datasets/coursera-ml/ex2/ex2data1.txt

   #+begin_src bash :var data=data
     head $data -n 3
   #+end_src

   #+RESULTS:
   | 34.62365962451697 |  78.0246928153624 | 0 |
   | 30.28671076822607 | 43.89499752400101 | 0 |
   | 35.84740876993872 | 72.90219802708364 | 0 |


   #+begin_src python :var data=data
     data = np.loadtxt(data, delimiter=",")
     X, y = data[:, 0:2], data[:, 2]
     X = np.c_[np.ones(len(X)), X]

     def assert_with_answer(answer, gt, epsilon=0.0001):
         return pytest.approx(answer, epsilon) == gt
   #+end_src

   #+RESULTS:


   #+begin_src python
     def plot_data(X, y, label_x, label_y, label_pos, label_neg):
         neg = (y == 0).ravel()
         pos = (y == 1).ravel()

         # この関数をがっしりとするために, 以下のコードを使用
         # axes = plt.gca()            # 現在の (あるいは新しく作成した) axisを獲得 
         plt.scatter(X[pos, 1], X[pos, 2],
                      marker="+", c="k", s=60, linewidth=2, label=label_pos)
         plt.scatter(X[neg, 1], X[neg, 2],
                      c="y", s=60, label=label_neg)
         plt.xlabel(label_x)
         plt.ylabel(label_y)
         plt.legend(frameon=True, fancybox=True)
   #+end_src

   #+RESULTS:


   #+begin_src python :file ../output/images/logistic-regression-coursera.png
     plot_data(X, y, "Exam 1 score", "Exam 2 score", "Admitted", "Not admitted")
   #+end_src

   #+RESULTS:
   [[file:../output/images/logistic-regression-coursera.png]]
** 関数
   ロジスティック関数の仮説関数.
   #+begin_src python
     def sigmoid(X):
         return 1 / (1 + np.exp(-X))

     def h(X, theta):
         return sigmoid(X @ theta)

     test = [0, 0.5]
     print(sigmoid(test[0]) == test[1])
     print(sigmoid(-1) < test[1])
     print(sigmoid(1) > test[1])
   #+end_src

   #+RESULTS:
   : True
   : True
   : True


   #+begin_src python
     def cost_function(theta, X, y):
         m = y.size
         J = 0
         out = h(X, theta)

         J = ( (-y * np.log(out)) -
               ((1 - y) * np.log(1 - out)) ).sum() / m

         if np.isnan(J):
             return np.inf
         return J

     def gradient(X, y, theta, size):
         return (X.T @ (h(X, theta) - y)) / size

     def cost_gradient(theta, X, y, J_history=False):
         if J_history:
             global opt_J_history
         grad = gradient(X, y, theta, len(X))
         J = cost_function(theta, X, y)
         if J_history:
             opt_J_history.append(J)
         return J, grad
   #+end_src

   #+RESULTS:


   テスト
   #+begin_src python
     # zero theta
     initial_theta = np.zeros(X.shape[1])
     cost, grad = cost_gradient(initial_theta, X, y)

     print(assert_with_answer(cost, 0.693, 0.001))
     print(assert_with_answer(grad, [-0.1000, -12.0092, -11.2628]))

     # non-zero theta
     initial_theta = np.array([-24, 0.2, 0.2])
     cost, grad = cost_gradient(initial_theta, X, y)

     print(assert_with_answer(cost, 0.218, 0.01))
     print(assert_with_answer(grad, [0.043, 2.566, 2.647], 0.01))
   #+end_src

   #+RESULTS:
   : True
   : True
   : True
   : True


   #+begin_src python
     def my_gradient_descent(theta, X, y, alpha=0.01, epoch=400):
         # batch gradient descent (すべてのデータを使用)
         J_history = []
         size = y.size
         for i in range(epoch):
             grad = gradient(X, y, theta, size)
             theta[:] -= alpha * grad

             J = cost_function(theta, X, y)
             J_history.append(J)

         return theta, J_history
   #+end_src

   #+RESULTS:

** 最適化による学習と評価
   今回,「linear-regression.org」と違って,「gradient_descent」を少なくとも直接に使用しない.
   #+begin_src python
     options = {"maxiter": 400}
     initial_theta = np.zeros(X.shape[1])
     # method=grad_function にする場合, `cost_gradient` は J のみ出すようにできるみたい.
     # 「Legacy」を参加すること.
     opt_J_history = []
     res = optimize.minimize(cost_gradient,
                             initial_theta,
                             (X, y, True),
                             jac=True,
                             method="TNC",
                             options=options)
     cost = res.fun
     theta = res.x

     print(assert_with_answer(cost, 0.203, 0.01))
     print(assert_with_answer(theta, [-25.161, 0.206, 0.201], 0.01))
   #+end_src

   #+RESULTS:
   #+begin_example
     True
     True
   #+end_example

   評価
   #+begin_src python
     y_hat = np.where(h(X, theta) >= .5, 1, 0)
     print(assert_with_answer(
         (y_hat == y).mean() * 100,
         89.00
     ))
   #+end_src

   #+RESULTS:
   : True

   1 点データに対する推測
   #+begin_src python
     data_point = np.array([1, 45, 85])
     print(assert_with_answer(
         h(data_point, theta),
         0.775,
         0.002                       # ちゃんと参考に定義されるeps
     ))
   #+end_src

   #+RESULTS:
   : True

** 自分の最急降下
   #+begin_src python
     initial_theta = np.zeros(X.shape[1])
     mgd_theta, J_history = my_gradient_descent(initial_theta, X, y, 0.001, 2_000_000)
   #+end_src

   #+RESULTS:

   評価
   #+begin_src python
     y_hat = np.where(h(X, mgd_theta) >= .5, 1, 0)
     print(assert_with_answer(
         (y_hat == y).mean() * 100,
         89.00
     ))
   #+end_src

   #+RESULTS:
   : True
   
** 可視化
   - 仮説: θ0 + θ1x1 + θ2x2 = 0
     - x2 をゼロのとき, 得た θ で x1 を計算することによって
       decision boundry を算出
   
   #+begin_src python :file ../output/images/logistic-regression-coursera-with-line-and-data-point.png
     # Decision Boundry ############################################################
     calc_x1 = lambda point: -((theta[0] + theta[2] * point) / theta[1])
     x2_points = [0, X[:, 2].max()]
     x1_points = []
     # x2 は _ のときに, x1 は
     for ii in x2_points:
         x1_points.append(calc_x1(ii))

     plt.plot(x1_points, x2_points, label="Desision boundry")

     # `data` のプロット #############################################################
     plt.scatter(data[1], data[2], marker="o", s=60, label=str(data_point))

     # All data ####################################################################
     plot_data(X, y, "Exam 1 score", "Exam 2 score", "Admitted", "Not admitted")
   #+end_src

   #+RESULTS:
   [[file:../output/images/logistic-regression-coursera-with-line-and-data-point.png]]


   自分の降下法による J_history 対 optimize.minimize.
   #+begin_src python :file ../output/images/mgd-vs-optmin.png
     plt.plot(J_history, label=my_gradient_descent.__name__)
     plt.plot(opt_J_history, label=optimize.minimize.__name__)
     plt.gca().set_xscale("log")
     plt.legend()
     plt.show()
   #+end_src

   #+RESULTS:
   [[../output/images/mgd-vs-optmin.png]]
   
* ロジスティック回帰 + 正規化 (Regularization)
* 質問 [0/1]
  - [ ] 活性関数?と仮説が違っている?
        仮説定義はアルゴリズム全体?
        線形回帰なら活性関数を使用しないと事?
  - [ ] [[自分の最急降下]]
    optimize.minimize を使用して, 全て正しいと思うが (全て参考に従うため),
    典型的な BGD を使用したら, 2 百万のエポックまで必要.
    optimize.minimize は 37 ぐらいのエポック(?)だけ必要そうから,
    自分の BGD の実装は正しい?

* 参考
  - https://github.com/wavelets/ml-coursera-python-assignments/blob/master/Exercise2/exercise2.ipynb
