#+title: General Note
#+author: Mohd Hafiz Hilman

機械学習・深層学習の一般的なノートのコレクション.

* 勾配喪失・爆発を防ぐための手法
  - [[file:layers/batch-normalization.org][Batch Normalization]]
  - ネットワークの重みの初期値を事前学習
  - 活性化関数を変更する [[file:activation_functions/relu.org][(ReLU]] など)
  - 学習係数を下げる
  - ネットワークの自由度を制約 ([[file:layers/dropout.org][Dropout]] など)


* 内部の共変量シフト (Internal Covariate Shift)


** 共変量シフト
   - 一般的な定義:
     入力の分布が異なる現象を意味
   - 機械学習・パターンん認識の分野では:
     訓練データのサンプリングと予測データの入力の分布に偏りがあり, アルゴリズムが対応できなくなること


** 内部の共変量シフト
   - 意味: ディープネットワークでは, 深くなった隠れ層において各層と Activation ごとに入力分布が変わってしまうこと


** 参照
   - https://deepage.net/deep_learning/2016/10/26/batch_normalization.html


* Rules of Thumb
  *従来の CNN* を立ち上げるのに, 以下のルールに従ったら良いでしょう.


** 画像は正方形べきである
   - 線形代数最適化ライブラリを利用できるように
   - 一般的な正方形の大きさ: 32, 64, 96, 224, 227, 229


** 最初の CONV 層の後の入力層は 2 で何回も割り切れるべき
   - =F= と =S= を調整
   - 「2 で割り切れる」のことで, ネットワークの空間入力は効率的に POOL でダウンサンプリングできる


** CONV 層について

*** =F=
    - F=1
      - より高度な構造では局所の特徴 (local features) を学ぶために使用される
    - F=3, F=5
      - まだ小さなフィルタサイズと考えられる
      - 一般的に使用される
    - F=7, F=11
      - 画像が 200x200 より大きく, それを減らすために, ネットワークの最初の CONV 層に使用できる
      - しかし, その後, F の値を減らすべき


*** =S=
    - S=1
      - 一般的に使用される
    - S>=2
      - 入力画像が大きいため, それを減らすために最初の CONV 層に使用できる


*** zero-padding
    - わざと CONV 層経由で入力のサイズを減らしたくない限り,
      CONV 層の出力のサイズがその入力のサイズと同じくなるように, zero-padding を適応
    - 複数の CONV 層を重ねながら, zero-padding を適応することによって, 実践的に分類精度の向上が見られる



** POOL 層について
   - 一般的には, F=2, S=2 の Max Pooling が使用される
   - 画像のサイズを減らすため, ネットワークの最初の部分に F=3 の Pooling が使用されるかもしれない
   - F=3 より大きな F は非常に珍しい


** Batch Normalization について
   - ほとんどすべての状況で使用するべき
   - ACT の後に適応
   - softmax classifer の前に適応しない
     - 理由: softmax classifier まではネットワークがすでに特徴を学べたと考えられる
   - これは expensive operation の一方, ネットワークのトレーニングを安定させるため,
     ほかのハイパ・パラメータを調整するのに楽になる


** Dropout について
   - FC と FC の間によく適応される
     - 50% の確率値は一般的
     - ほぼ全てのネットワークにこれを適応すべき
   - POOL と CONV の間
     - 10%-25% の確率値で適応することをオススメ
     - CONV のローカル接続のため, DO はあまり効果的ではないが, オーバフィッティングを
       減らすのに役に立つ
